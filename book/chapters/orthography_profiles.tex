\chapter{Orthography profiles}
\label{orthography-profiles}

\section{Characterizing writing systems}
\label{characterizing-writing-systems}

The Unicode Standard offers a very detailed technical approach 
for characterizing writing systems computationally. As such, it is 
sometimes too complex for the day-to-day practice of many linguists, 
as exemplified by the need to understand the common pitfalls that we discussed in Chapters 
\ref{unicode-pitfalls} \& \ref{ipa-meets-unicode}. Therefore, in this section we propose some simple 
guidelines for linguists working in multilingual environments.

Our aims for adopting a Unicode-based
solution are: (i) to improve the consistency of the encoding of sources, (ii)
to transparently document knowledge about the writing system (including
transliteration), and (iii) to do all of that in a way that is easy and quick to
manage for many different sources with many different writing systems. The
central concept in our proposal is the \textsc{orthography profile}, a simple
delimited text file, that characterizes and documents a writing system.
We also offer basic implementations in Python and R to assist with the
production of such files, and to apply orthography profiles for consistency
testing, grapheme tokenization and transliteration. Not only can orthography
profiles be helpful in the daily practice of linguistics, they also succinctly
document the orthographic details of a specific source, and, as such, might
fruitfully be published alongside sources (e.g.~in digital archives). Also, in
high-level linguistic analyses in which the graphemic detail is of central
importance (e.g.~phonotactic or comparative-historical studies), orthography
profiles can transparently document the decisions that have been taken in the
interpretation of the orthography in the sources used.

Given these goals, Unicode Locales (see
Chapter~\ref{the-unicode-approach}) might seem like the ideal orthography
profiles. However, there are various practical obstacles preventing the use of
Unicode Locales in the daily linguistic practice, namely: (i) the
XML structure\footnote{\url{http://unicode.org/reports/tr35/}} is too verbose to easily and quickly produce or correct manually,
(ii) Unicode Locales are designed for a wide scope of information (like date
formats or names of weekdays) most of which is not applicable for documenting
writing systems, and (iii) most crucially, even if someone made the effort to
produce a technically correct Unicode Locale for a specific source at hand,
then it is well-nigh impossible to deploy the description. This is because a locale
description has to be submitted to and accepted by the Unicode Common Locale
Data Repository. The repository is (rightly so) not interested in descriptions
that only apply to a limited set of sources (e.g.~descriptions for only a single
dictionary).

The major challenge, then, is developing an infrastructure to identify the
elements that are individual graphemes in a source, specifically for the
enormous variety of sources using some kind of alphabetic writing system.
Authors of source documents (e.g.~dictionaries, wordlists, corpora) use a
variety of writing systems that range from their own idiosyncratic
transcriptions to already well-established practical or longstanding
orthographies. Although the IPA is one practical choice as a sound-based
normalization for writing systems (which can act as an interlingual pivot to
attain interoperability across writing systems), graphemes in each writing
system must also be identified and standardized if interoperability across
different sources is to be achieved. In most cases, this amounts to more than
simply mapping a grapheme to an IPA segment because graphemes must first be
identified in context (e.g.~is the sequence one sound or two sounds or both?)
and strings must be tokenized, which may include taking orthographic rules into
account (e.g.~a nasal sound may be transcribed as <n> when it appears between 
two vowels, but when it appears between a vowel and a consonant it becomes 
a nasalized vowel <Ṽ>).

In our experience, data from each source must be
individually tokenized into graphemes so that its orthographic structure can be 
identified and its contents can be extracted. To extract data for analysis, a
source-by-source approach is required before an orthography profile can be
created. For example, almost every available lexicon on the world's languages is
idiosyncratic in its orthography and thus requires lexicon-specific approaches
to identify graphemes in the writing system and to map graphemes to phonemes, if
desired.

Our key proposal for the characterization of a writing system is to use a
grapheme tokenization as an inter-orthographic pivot. Basically, any source
document is tokenized by graphemes, and only then a mapping to IPA (or any other
orthographic transliteration) is performed. An \textsc{orthography profile} then is a
description of the units and rules that are needed to adequately model a
graphemic tokenization for a language variety as described in a particular
source document. An orthography profile summarizes the Unicode (tailored)
graphemes and orthographic rules used to write a language (the details of the
structure and assumptions of such a profile will be presented in the next
section).

% TODO: add tsoshi figure here?

As an example of graphemic tokenization, note the three different levels of
technical and linguistic elements that interact in the hypothetical lexical
form <tsʰṍ̰shi>:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item code points (10 text elements): t s ʰ o \dia{0303} \dia{0330} \dia{0301} s h i 
	\item grapheme clusters (7 text elements): t s ʰ ṍ̰ s h i 
	\item tailored grapheme clusters (4 text elements): tsʰ ṍ̰ sh i 
\end{enumerate}

In (1), the string <tsʰṍ̰shi> has been tokenized into ten Unicode code points
(using NFD normalization), delimited here by space. Unicode normalization is
required because sequences of code points can differ in their visual and logical
orders. For example, <õ̰> is ambiguous to whether it is the sequence of <o> +
<\dia{0303}> + <\dia{0330}> or <o> + <\dia{0330}> + <\dia{0303}>. Although these two variants are visually homoglyphs,
computationally they are different (see Sections \ref{pitfall-ipa-homoglyphs} \& \ref{pitfall-homoglyphs-in-IPA}). 
Unicode normalization should be applied to
this string to reorder the code points into a canonical order, allowing the data
to be treated  for search and comparison. 

In (2), the
Unicode code points have been logically normalized and visually organized into
grapheme clusters, as specified by the Unicode Standard. The combining character
sequence <õ̰> is normalized and visually grouped together. Note that the
\textsc{modifier letter small h} at \uni{02B0} is not grouped with any 
other character. This is because it
belongs to the Spacing Modifier Letters category. The Unicode Standard 
does not specify the direction that these characters modify a host
character. For example, it can indicate either pre- or post-aspiration (whereas the
nasalization or creaky diacritic is defined in the Unicode Standard to apply to
a specified base character). 

Finally, to arrive at the graphemic tokenization in (3), tailored grapheme
clusters are needed, possibly as specified in an orthography profile. For example,
an orthography profile might specify that the sequence of characters <tsʰ> form
a single grapheme. The orthography profile could also specify orthographic
rules, e.g.~when tokenizing graphemes in English, the sequences <sh> in the
forms <mishap> and <mishmash> should be treated as distinct sequences depending
on their contexts.

\section{Informal description}
\label{informal-description-of-orthography-profiles}

An orthography profile describes the Unicode code points, characters, graphemes
and orthographic rules in a writing system. An orthography profile is a
language-specific (and often even resource-specific) description of the units
and rules that are needed to adequately model a writing system. An important
assumption is that we assume a resource is encoded in Unicode or
has been converted to Unicode. Any data source that the Unicode Standard is
unable to capture will also not be captured by an orthography profile.

Informally, an orthography profile specifies the graphemes -- in Unicode
parlance \textsc{tailored grapheme clusters} -- that are expected to occur in any
data to be analyzed or checked for consistency. These graphemes are first
identified throughout the whole data, a step which we call
\textsc{tokenization}, and simply returned as such, possibly including
error messages about any parts of the data that are not specified by the
orthography profile. Once the graphemes are identified, they might also be
changed into other graphemes -- a step which we call \textsc{transliteration}.
When a grapheme has different possible transliterations, then these differences
should be separated by contextual specification, possibly down to listing
individual exceptional cases.

The crucial difference between our current proposal and traditional
computational approaches to transliteration is the strict separation between
tokenization and transliteration. Most computational approaches to
transliteration are based on finite-state transducers (including the
transliteration as described in the Unicode Locale Data Markup
Language).\footnote{\url{http://www.unicode.org/reports/tr35/}} Finite-state 
transducers attempt to describe the mapping from input to output string directly 
as a set of rewrite rules. Although such systems are computationally well 
understood, we feel that they are not well-suited for day-to-day linguistic 
practice. First, by forcing a first step of grapheme tokenization, our system 
tries to keep close to the logic of the writing system. Second, by separating 
tokenization from transliteration there is no problem with `feeding' and 
`bleeding' of rules, common with transducers (cf.~Section~\ref{r-implementation}).

Note that to deal with ambiguous parsing cases, it is still possible to use the
Unicode approach of including the \textsc{zero-width non-joiner} character at
\uni{200C} into the text. The idea is to add this character into the text to
identify cases in which a sequence of characters is \textit{not} supposed to be
a complex grapheme cluster -- even though the sequence is in the orthography
profile.

In practice, we foresee a workflow in which orthography profiles are iteratively
refined, while at the same time inconsistencies and errors in the data to be
tokenized are corrected. In some more complex use cases there might even be a
need for multiple different orthography profiles to be applied in sequence (see
Sections~\ref{python-implementations} \& \ref{r-implementation} on various exemplary use cases). The result of any such
workflow will normally be a cleaned dataset and an explicit description of the
orthographic structure in the form of an orthography profile. Subsequently, the
orthography profiles can be easily distributed in scholarly channels alongside
the cleaned data, for example in supplementary material added to journal papers
or in electronic archives.

\section{Formal specification}
\label{formal-specification-of-orthography-profiles}

\subsection*{File Format}
The formal specifications of an orthography profile (or simply \textsc{profile}
for short) are the following:

\begin{enumerate}
	\def\labelenumi{A\arabic{enumi}.} 
	\item \textsc{A profile is a unicode utf-\scalebox{0.8}{8} encoded text file} 
	   % normalized following NFC (or NFD if specified in the metadata)
       that includes information pertinent to the orthography.\footnote{See 
	   Section~\ref{pitfall-file-formats} in which we suggest to use NFC, 
	   no-BOM and LF line breaks because of the pitfalls they avoid. A keen reviewer notes, however, that specifying 
	   a convention for line endings and BOM is overly strict because most 
	   computing environments (now) transparently handle both alternatives. 
	   For example, using Python a file can be decoded using the encoding 
	   ``utf-8-sig'', which strips away the BOM (if present) and reads 
	   an input full in text mode, so that both line feed variants ``LF'' and 
	   ``CRLF'' will be stripped.}

	\item \textsc{A profile is a delimited text file with an obligatory header
       line}. A minimal profile must have a single column with the header \texttt{Grapheme}. 
	   For any additional columns, the name in the header 
       must be specified. The actual ordering of the columns is unimportant. 
	   The header list must be delimited 
	   in the same way as the rest of the file's contents. Each record must be kept 
	   on a separate line. Separate lines with comments are not allowed. Comments that
       belong to specific lines must be put in a separate column of
       the file, e.g.~add a column called \textsc{comments}.

\begin{comment}	   
	\item \textsc{Metadata should be added in a separate utf-\scalebox{0.8}{8} text file} using a basic
       \textsc{tag: value} format. Metadata about the orthographic description
       given in the orthography profile includes, minimally, (i) author, (ii)
       date, (iii) title of the profile, (iv) a stable language identifier
       encoded in BCP 47/ISO 639-3 of the target language of the profile, and (v)
       bibliographic data for resource(s) that illustrate the orthography
       described in the profile. Further, (vi) the tokenization method and (vii)
       the Unicode normalization used should be documented here (see below).
\end{comment}

	\item \textsc{Metadata should be added in a separate utf-\scalebox{0.8}{8} text file} using 
	   the JSON-LD dialect specified in \textit{Metadata Vocabulary for Tabular 
	   Data}.\footnote{\url{https://www.w3.org/TR/tabular-metadata/}} This metadata 
	   format allows for easy inclusion of Dublin Core metadata,\footnote{\url{http://dublincore.org/}} 
	   which should be used to specify information about the orthographic description 
	   in the orthography profile.\footnote{\url{http://w3c.github.io/csvw/metadata/\#dfn-common-property}} 
	   The orthography profile metadata should minimally include provenance information including: 
	   (i) author, (ii) date, (iii) title of the profile, and (iv)
       bibliographic data for resource(s) that illustrate the orthography
       described in the profile. Crucially, the metadata should also specify 
	   (v) a stable language identifier of the target language of the profile
       using BCP 47/ISO 639-3 or Glottocode as per the CLDF ontology.\footnote{\url{http://cldf.clld.org/v1.0/terms.rdf}}	   	   
	   Further, the metadata file should provide information about the orthography profile's structure and contents, 
	   including: (vi) its dialect description,\footnote{\url{http://w3c.github.io/csvw/metadata/\#dfn-dialect-descriptions}} 
	   and (vii) proper column descriptions,\footnote{\url{http://w3c.github.io/csvw/metadata/\#dfn-datatype-description}} 
	   which describe how a column should be interpreted and processed (e.g.\ whether they 
	   should be processed as regular expressions; see below).
	   Finally, in accordance with the \textit{Metadata Vocabulary for Tabular 
	   Data}, the metadata's filename should consist of the orthography 
	   profile's filename appended with ``-metadata.json''.\footnote{JSON-LD metadata 
	   is also the choice for datasets conforming to the Cross-Linguistic Data Formats standard, 
	   see: \url{http://cldf.clld.org/}.}

\end{enumerate}

\noindent The content of a profile consists of lines, each describing a grapheme
of the orthography, using the following columns:

\begin{enumerate}
	\def\labelenumi{A\arabic{enumi}.} \setcounter{enumi}{4} 
	\item \textsc{A minimal profile consists of a single column} with a header
       called \texttt{Grapheme}, listing each of the different graphemes in a
       separate line. The name of this column is crucial for automatic 
       processing.
	\item \textsc{Optional columns can be used to specify the left and right
       context of the grapheme}, to be designated with the headers \texttt{Left}
       and \texttt{Right} respectively. The same grapheme can occur multiple
       times with different contextual specifications, for example to
       distinguish different pronunciations depending on the context. 
	\item \textsc{The columns \texttt{Grapheme}, \texttt{Left} and \texttt{Right}
       can use regular expression \\ metacharacters.} If regular expressions are
       used, then they must be specified in the metadata file as such, 
	   and all literal usage of the special symbols, like full stops <.>
       or dollar signs <\$> (so-called \textsc{metacharacters}) have to be
       explicitly escaped by adding a backslash before them (i.e.~use
       <\textbackslash.> or <\textbackslash\$>). Note that any specification of
       context automatically expects regular expressions, so it is 
       better to always escape all regular expression metacharacters when used
       literally in the orthography. The following symbols will need to be
       preceded by a backslash: {[} {]} ( ) \{ \} | ~+ * . - ! ? \^{} \$ and the
       backslash \textbackslash~itself. 
	\item \textsc{An optional column can be used to specify classes of graphemes},
       to be identified by the header \texttt{Class}. For example, this column
       can be used to define a class of vowels. Users can simply add ad-hoc
       identifiers in this column to indicate a group of graphemes, which can
       then be used in the description of the graphemes or the context. The
       identifiers should of course be chosen so that they do not conflate
       with any symbols used in the orthography. Note that such
       classes only refer to the graphemes, not to the context. 
	\item \textsc{Columns describing transliterations for each graphemes can be
       added and named at will}. Often more than a single possible
       transliteration will be of interest. Any software application using these
       profiles should prompt the user to name any of these columns to select a
       specific transliteration. 
	\item \textsc{Any other columns can be added freely, but will be typically ignored
       by any software application using the profiles}. As orthography profiles
       are also intended to be read and interpreted by humans, it is often
       very useful to add extra information about the graphemes in further
       columns, such as Unicode code points, Unicode names, frequency of
       occurrence, examples of occurrence, explanation of contextual
       restrictions, or comments. 
 \end{enumerate}

\subsection*{Processing}
For the automated processing of the profiles, the following technical standards
will be expected:

\begin{enumerate}
	\def\labelenumi{B\arabic{enumi}.} 
	\item \textsc{Each line of a profile will be interpreted according to the content type of the column as specified in the profile metadata}. Content types include literal and regular expression.
	\item \textsc{The \textsc{class} column will be used to produce explicit
       \textsc{or} chains of regular expressions}, which will then be inserted
       in the \texttt{Grapheme}, \texttt{Left} and \texttt{Right} columns at
       the position indicated by the class-identifiers. For example, a class
       called \texttt{V} as a context specification might be replaced by a regular
       expression like:
       \texttt{(au\textbar{}ei\textbar{}a\textbar{}e\textbar{}i\textbar{}o\textbar{}u}).
       Only the graphemes themselves are included here, not any contexts
       specified for the elements of the class. Note that the 
       ordering inside this regular expression is crucial (e.g.\ regular expressions are greedy, so
	   longest matches should be placed before matching substrings).
	\item \textsc{The \textsc{left} and \textsc{right} contexts will be included
       into the regular expressions by using lookbehind and lookahead}.
       Basically, the actual regular expression syntax of lookbehind and
       lookahead is simply hidden to the users by allowing them to only specify
       the contexts themselves. Internally, the contexts in the columns
       \texttt{Left} and \texttt{Right} are combined with the column
       \texttt{Grapheme} to form a complex regular expression like:\\ 
       \texttt{(?\textless{}=Left)Grapheme(?=Right)}. 
	\item \textsc{The regular expressions will be applied in the order as specified
       in the profile, from top to bottom.} A software implementation can offer
       help in figuring out the optimal ordering of the regular expressions, but
       then it should be made explicit in the orthography profile because regular 
	   expressions are executed in order from top to bottom.
\end{enumerate}

\noindent The actual implementation of the profile on some text-string will function as
follows:

\begin{enumerate}
	\def\labelenumi{B\arabic{enumi}.} \setcounter{enumi}{4} 
	\item \textsc{All graphemes are matched in the text before they are tokenized
       or transliterated}. In this way, there is no necessity for the user to
       consider \textit{feeding} and \textit{bleeding} situations, in which the application of
       a rule either changes the text so another rule suddenly applies (feeding)
       or prevents another rule from applying (bleeding). 
	\item \textsc{The matching of the graphemes can occur either globally or
       linearly.} From a computer science perspective, the most natural way to
       match graphemes from a profile in some text is by walking linearly
       through the text-string from left to right, and at each position going
       through all graphemes in the profile to see which one matches, then go to
       the position at the end of the matched grapheme and start over. This is
       basically how a finite state transducer works, which is a
       well-established technique in computer science. However, from a
       linguistic point of view, our experience is that most linguists find it
       more natural to think from a global perspective. In this approach, the
       first grapheme in the profile is matched everywhere in the text-string
       first, before moving to the next grapheme in the profile. Theoretically,
       these approaches will lead to different results, though in practice of
       actual natural language orthographies they almost always lead to the same
       result. Still, we suggest that any software application using orthography
       profiles should offer both approaches (i.e.\ \textsc{global} or
       \textsc{linear}) to the user. The approach used should be documented in
       the metadata as \textsc{tokenization method}. 
	\item \textsc{The matching of the graphemes can occur either in nfc or nfd.} 
	   The Unicode Standard states that software is free to compose or decompose the character stream from one 
	   representation to another. However, Unicode conformant software must treat canonically equivalent sequences in 
	   NFC and NFD as the same. It is up to the orthography profile creator how they choose to encode their profile. 
	   Several sources suggest to use NFC when possible for text encoding,\footnote{\url{http://www.win.tue.nl/~aeb/linux/uc/nfc_vs_nfd.html}} 
	   including SIL International with regard to data archiving.\footnote{\url{http://scripts.sil.org/cms/scripts/page.php?item_id=NFC_vs_NFD}} 
	   In our experience, in some use cases it turns out to
       be practical to treat both text and profile as NFD. This typically
       happens when many different combinations of diacritics occur in the
       data. An NFD profile can then be used to first check which individual
       diacritics are used, before turning to the more cumbersome inspection of
       all combinations. We suggest that any software application using
       orthography profiles should offer both approaches (i.e.\ \textsc{NFC} or
       \textsc{NFD}) to the user. The approach used can be documented in the
       metadata as \textsc{unicode normalization}. 
	\item \textsc{The text-string is always returned in tokenized form} by
       separating the matched graphemes by a user-specified symbols-string. Any
       transliteration will be returned on top of the tokenization. 
	\item \textsc{Leftover characters}, i.e.~\textsc{characters that are not
       matched by the profile, should be reported to the user as errors.}
       Typically, the unmatched characters are replaced in the tokenization by a
       user-specified symbol-string.       
 \end{enumerate}

\subsection*{Software applications}

Any software application offering to use orthography profile:

\begin{enumerate}
	\def\labelenumi{\arabic{enumi}.} 
	\item \textsc{should offer user-options} to specify:
	\begin{enumerate}
		\def\labelenumii{C\arabic{enumii}.} 
		\item \textsc{the name of the column to be used for transliteration} (if any). 
		\item \textsc{the symbol-string to be inserted between graphemes.} Optionally,
        a warning might be given if the chosen string includes characters from
        the orthography itself. 
		\item \textsc{the symbol-string to be inserted for unmatched strings} in the
        tokenized and transliterated output. 
		\item \textsc{the tokenization method}, i.e.~whether the tokenization should
        proceed as \textsc{global} or \textsc{linear} (see B6 above). 
		\item \textsc{unicode normalization}, i.e.~whether the text-string and profile
        should use \textsc{NFC} or \textsc{NFD}. 
    \end{enumerate}
	\item \textsc{might offer user-options}:
	\begin{enumerate}
		\def\labelenumii{C\arabic{enumii}.} \setcounter{enumii}{5} 
		\item \textsc{to assist in the ordering of the graphemes.} In our experience 
		working with idiosyncratic transcriptions and orthographies from low-resource languages, it
        is helpful to identify multi-sequence graphemes before single graphemes, and to
        identify graphemes with context before graphemes without context. Further,
        frequently relevant rules might be applied after rarely relevant rules
        (though frequency is difficult to establish in practice, as it depends
        on the available data). Also, if this all fails to give any decisive
        ordering between rules, it seems useful to offer linguists the option to
        reverse the ordering from any manual specified ordering, because
        linguists tend to write the more general rule first, before turning to
        exceptions or special cases. 
		\item \textsc{to assist in dealing with upper and lower case characters.} It
        seems practical to offer some basic case matching, so characters like
        <a> and <A> are treated equally. This will be useful in many concrete
        cases (such as search or collation), although the user should be warned that case matching does not
        function universally in the same way across orthographies.\footnote{For example 
		compare the different first-letter capitalization practices of the digraphs 
		<ǋ> and <Ĳ> (single-character ligatures in the Unicode Standard) 
		in the Latin-based scripts of Southern-Slavic languages and 
		Dutch, respectively.} Ideally,
        users should prepare orthography profiles with all lowercase and
        uppercase variants explicitly mentioned, so by default no case matching
        should be performed. 
		\item \textsc{to treat the profile literally}, i.e.~to not interpret regular
        expression metacharacters. Matching graphemes literally often leads to
        significant speed increase, and ensures that users do not have to worry
        about escaping metacharacters. However, in our experience all actually
        interesting use cases of orthography profiles include some contexts,
        which automatically prevents any literal interpretation.
    \end{enumerate}
	\item \textsc{should return the following information} to the user:
	\begin{enumerate}
		\def\labelenumii{C\arabic{enumii}.} \setcounter{enumii}{8} 
		\item \textsc{the original text-strings} to be processed in the specified
        Unicode normalization, i.e.~in either NFC or NFD as specified by the
        user. 
		\item \textsc{the tokenized strings}, with additionally any transliterated
        strings, if transliteration is requested. 
		\item \textsc{a survey of all errors encountered}, ideally both (i) in which
        text-strings any errors occurred and (ii) which characters in the
        text-strings lead to errors. 
		\item \textsc{a reordered profile}, when any automatic reordering is offered. 
	\end{enumerate}
\end{enumerate}

