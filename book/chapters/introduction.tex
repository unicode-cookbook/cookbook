\chapter{Writing systems}
\label{writing_systems}

% ==========================
\section{Introduction}
\label{introduction}
% ==========================

Writing systems arise and develop in a complex mixture of cultural,
technological and practical pressures. They tend to be highly conservative, in
that people who have learned to read and write in a specific way -- however
impractical or tedious -- are mostly unwilling to change their habits. Writers
tend to resist spelling reforms. In all literate societies there exists a strong
socio-political mainstream that tries to force unification of writing (for
example by strongly enforcing ``right'' from ``wrong'' spelling in schools).
However, there are also communities of users who take as many liberties in
their writing as they can get away with.

For example, the writing of tone diacritics in Yoruba is often proclaimed to be
the right way to write, although many users of Yoruba orthography seem to be
perfectly fine with leaving them out. As pointed out by the proponents of the
official rules, there are some homographs when leaving out the tone diacritics
\citep[44]{Olumuyiw2013}. However, writing systems (and the languages they
represent) are normally full of homographs (and homophones), which is 
not a problem at all for speakers of the language. More importantly, writing is
not just a purely functional tool, but just as importantly it is a mechanism to
signal social affiliation. By showing that you \textit{know the rules} of
expressing yourself in writing, others will more easily accept you as a worthy
participant in their group -- whether it means following the official rules 
when writing a job application or conforming to the informal rules when writing text 
messages. The case of Yoruba writing is
an exemplary case, as even after more than a century of efforts to standardize
the writing systems, there is still a wide range of variation of writing in
daily use \citep{Olumuyiw2013}.

\subsubsection*{Formalizing orthographic structure}

The resulting cumbersome and often illogical structure of writing systems, and
the enormous variability of existing writing systems for the world's languages, 
is a fact of life that scholars have to accept and they should try to adapt to as well as
they can. Our goal in this book is a proposal for how to do exactly that: formalize 
knowledge about individual writing systems in a form that is easy to use 
for linguists in daily practice, and at the same time computer-readable for 
automated processing.

When considering worldwide linguistic diversity, including the many
lesser-studied and endangered languages, there exist numerous different
orthographies using symbols from the same scripts. For example, there are
hundreds of orthographies using Latin-based alphabetic scripts. All of these
orthographies use the same symbols, but these symbols differ in meaning and
usage throughout the various orthographies. To be able to computationally use
and compare different orthographies, we need a way to specify all orthographic
idiosyncrasies in a computer-readable format. We call such specifications
\textsc{orthography profiles}. Ideally, these specifications have to be
integrated into so-called Unicode Locales,\footnote{\url{http://cldr.unicode.org/locale_faq-html}} 
though we will argue that in practice this is often not the most useful solution for the kind of problems
arising in the daily practice of many linguists. Consequently, a central goal of this book is to flesh out the linguistic-specific challenges regarding Unicode Locales and to work out suggestions to simplify their structure for usage in a linguistic context. Conversely, we also aim to improve linguists' understanding and appreciation for the accomplishments of the Unicode Consortium in the development of the Unicode Standard.

The need to use computational methods to compare different orthographies arises most
forcefully in the context of language comparison. Concretely, the proper
processing of orthographies and transcription systems becomes critical for the
development of quantitative methods for language comparison and historical
reconstruction. In order to investigate worldwide linguistic variation and to
model the historical and areal processes that underlie linguistic
diversity, it is crucial that we are able to flexibly process numerous
resources in different orthographies. In many cases even different resources
on the same language use different orthographic conventions. Another
orthographic challenge that we encounter regularly in our linguistic practice is
electronic resources on a particular language that claim to follow a specific
orthographic convention (often a resource-specific convention), but on closer
inspection such resources are almost always not consistently encoded. Thus, a
second goal of our orthography profiles is to allow for an easy specification of
orthographic conventions, and use such profiles to check consistency and to
report errors to be corrected.

% \footnote{cf.~\citet{Steiner_etal2011,List2012,List2012a,ListMoran2013,MoranProkic2013}}

A central step in our proposed solution to this problem is the tailored grapheme
separation of strings of symbols, a process we call \textsc{grapheme
tokenization}. Basically, given some strings of symbols (e.g.~morphemes, words,
sentences) in a specific source, our first processing step is to specify how
these strings should be separated into graphemes, considering the specific
orthographic conventions used in a particular source document. Our experience is
that such a graphemic tokenization can often be performed reasonably accurately
without extensive in-depth knowledge about the phonetic and phonological details
of the language in question. For example, the specification that <ou> is a
grapheme of English is a much easier task than to specify what exactly the
phonetic values of this grapheme are in any specific occurrence in English
words. Grapheme separation is a task that can be performed relatively reliably
and with limited time and resources (compare, for example, the
daunting task of creating a complete phonetic or phonological normalization).

Although grapheme tokenization is only one part of the solution, it is an
important and highly fruitful processing step. Given a grapheme tokenization,
various subsequent tasks become easier, for instance (a) temporarily reducing the
orthography in a processing pipeline, e.g.~only distinguishing high versus low
vowels; (b) normalizing orthographies across sources (often including temporary
reduction of oppositions), e.g.~specifying an (approximate) mapping to the
International Phonetic Alphabet; (c) using co-occurrence statistics across
different languages (or different sources in the same language) to estimate the
probability of grapheme matches, e.g.~with the goal to find regular sound
changes between related languages or transliterations between different sources
in the same language.

\subsubsection*{Structure of this book}

Before we deal with these proposals we will first discuss the theoretical
background on text encoding, on the Unicode Standard, and on the International
Phonetic Alphabet. In the remainder of this chapter, we give an extended
introduction to the notion of encoding (Section~\ref{encoding}) and the
principles of writing systems from a linguistic perspective
(Section~\ref{linguistic-terminology}). In Chapter~\ref{the-unicode-approach}, we 
discuss the notions of encoding and writing systems from the perspective of the Unicode
Consortium. We consider the Unicode
Standard to be a breakthrough (and ongoing) development that fundamentally
changed the way we look at writing systems, and we aim to provide here a
slightly more in-depth survey of the many techniques that are available in the
standard. A good appreciation for the solutions that the Unicode Consortium has created 
allows for a thorough understanding of the possible pitfalls that one might
encounter when using the Unicode Standard in general (Chapter~\ref{unicode-pitfalls}). Linguists are more often interested in using the Unicode Standard with the International Phonetic Alphabet (IPA). We first provide a history of the development of the IPA and early attempts to encode it electronically (Chapter~\ref{the-international-phonetic-alphabet}) before we discuss the rather problematic marriage of the IPA with the Unicode Standard (Chapter~\ref{ipa-meets-unicode}).

In the second part of the book (Chapters~\ref{practical-recommendations}, \ref{orthography-profiles} \&
\ref{implementation}) we describe our proposals of how to deal with the Unicode
Standard in the daily practice of (comparative) linguistics. First, we provide some 
practical recommendations for using the Unicode Standard and IPA for ordinary 
working linguists and for computer programmers (Chapter~\ref{practical-recommendations}). 
Second, we discuss the
challenges of characterizing a writing system; to solve these problems, we
propose the notions of orthography profiles, closely related to Unicode locale
descriptions (Chapter~\ref{orthography-profiles}). Lastly, we provide an introduction to two open source libraries that we have developed, in Python and R, for working with linguistic data and orthography profiles (Chapter~\ref{implementation}).


\subsubsection*{Conventions}

The following conventions are adhered to in this book. All phonemic and phonetic
representations are given in the International Phonetic Alphabet (IPA), unless
noted otherwise \citep{IPA2015}. Standard conventions are used for
distinguishing between graphemic < >, phonemic / / and phonetic [ ]
representations. For character descriptions, we follow the notational
conventions of the Unicode Standard \citep{Unicode2018}. Character names are
represented in small capital letters (e.g.~\textsc{latin small letter schwa})
and code points are expressed as U\emph{+n}, where \emph{n} is a four to six
digit hexadecimal number, e.g.~\uni{0256}, which can be rendered as the glyph <ə>.

% ==========================
\section{Encoding}
\label{encoding}
% ==========================

There are many in-depth histories of the origin and development of writing
systems \citep[e.g.~][]{Robinson1995,Powell2012}, a story that we therefore will
not repeat here. However, the history of turning writing into machine-readable
code is not so often told, so we decided to offer a short survey of the major
developments of such encoding here.\footnote{Because of the recent
history as summarized in this section, we have used mostly rather ephemeral
internet sources. When not referenced by traditional literature in the
bibliography, we have used \url{http://www.unicode.org/history/} and various
Wikipedia pages for the information presented here. A useful survey of the
historical development of the physical hardware of telegraphy and
telecommunication is \citet{Huurdeman2003}. Most books that discuss the
development of encoding of telegraphic communication focus of cryptography,
e.g.~\citet{Singh1999}, and forego the rather interesting story of open,
i.e.~non-cryptographic, encoding that is related here.} This history turns out
to be intimately related to the history of telegraphic communication.

\subsubsection*{Telegraphy}

Writing systems have existed for roughly 6000 years, allowing people to exchange
messages through time and space. Additionally, to quickly bridge large geographic
distances, telegraphic systems of communication (from Greek \emph{τῆλε
γράφειν} `distant writing') have a long and widespread history since ancient
times. The most common telegraphic systems worldwide are so-called whistled
languages \citep{Meyer2015}, but also drumming languages \citep{Meyer2012} and
signaling by smoke, fire, flags, or even changes in water levels through
hydraulic pressure have been used as forms of telegraphy. 

Telegraphy was reinvigorated at the end of the eighteenth century through the
introduction of so-called semaphoric systems by Claude Chapelle to convey
messages over large distances. Originally, various specially designed
contraptions were used to send messages. Today, descendants of these systems are
still in limited use, for example utilizing flags or flashing lights. The
innovation of those semaphoric systems was that all characters of the
written language were replaced one-to-one by visual signals. Since then, all
telegraphic systems have adopted this principle,\footnote{Sound- and
video-based telecommunication take a different approach by ignoring
the written version of language and they directly encode sound waves or light
patterns.} namely that any language to be
transmitted first has to be turned into some orthographic system, which
subsequently is encoded for transmission by the sender, and then turned back
into orthographic representation at the receiver side. This of course implies 
that the usefulness of any such telegraphic
encoding completely depends on the sometimes rather haphazard structure of
orthographic systems.

In the nineteenth century, electric telegraphy led to a further innovation in
which written language characters were encoded by signals sent through a copper
wire. Originally, \textsc{bisignal codes} were used, consisting of two different
signals. For example, Carl Friedrich Gauss in 1833 used positive and negative
current \citep[282]{Mania2008}. More famous and influential, Samuel Morse in
1836 used long and short pulses. In those bisignal codes each character from the
written language was encoded with a different number of signals (between one and
five), so two different separators are needed: one between signals and one
between characters. For example, in Morse-code there is a short pause between
signals and a long pause between characters.\footnote{Actually, Morse-code also
includes an extra long pause between words. Interestingly, it took a long time
to consider the written word boundary -- using white-space -- as a bona-fide
character that should simply be encoded with its own code point. This happened
only with the revision of the Baudot-code (see below) by Donald Murray in 1901,
in which he introduced a specific white-space code. This principle has been
followed ever since.}

\subsubsection*{Binary encoding}

From those bisignal encodings, true \textsc{binary codes} developed with a fixed
length of signals per character. In such systems only a single separator between
signals is needed, because the separation between characters can be established
by counting until a fixed number of signals has passed.\footnote{Of course, no
explicit separator is needed at all when the timing of the signals is known, which is
the principle used in all modern telecommunication systems. An important modern
consideration is also how to know where to start counting when you did not catch
the start of a message, something that is known in Unicode as \textsc{self
synchronization}.} In the context of electric telegraphy, such a binary code
system was first established by Émile Baudot in 1870, using a fixed combination
of five signals for each written character.\footnote{True binary codes have a
longer history, going back at least to the Baconian cipher devised by Francis
Bacon in 1605. However, the proposal by Baudot was the quintessential proposal
leading to all modern systems.} There are $2^5 = 32$ possible combinations when
using five binary signals; an encoding today designated as 5-bit. These
codes are sufficient for all Latin letters, but of course they do not suffice
for all written symbols, including punctuation and digits. As a solution, the
Baudot code uses a so-called shift character, which signifies that from
that point onwards -- until shifted back -- a different encoding is used, allowing
for yet another set of 32 codes. In effect, this means that the Baudot code, and
the \textsc{International Telegraph Alphabet} (ITA) derived from it, had an
extra bit of information, so the encoding is actually 6-bit (with $2^6
= 64$ different possible characters). For decades, this encoding was the
standard for all telegraphy and it is still in limited use today.

To also allow for different uppercase and lowercase letters and for a large
variety of control characters to be used in the newly developing technology of
computers, the American Standards Association decided to propose a new 7-bit
encoding in 1963 (with 2\textsuperscript{7} = 128 different possible characters), 
known as the
\textsc{American Standard Code for Information Interchange} (ASCII), geared
towards the encoding of English orthography. With the ascent of other
orthographies in computer usage, the wish to encode further variations of Latin
letters (including German <ß> and various letters with diacritics, e.g.\ <è>) led the
Digital Equipment Corporation to introduce an 8-bit \textsc{Multinational
Character Set} (MCS, with 2\textsuperscript{8} = 256 different possible characters), first used with the introduction of the VT{\large 220} Terminal in 1983. 

Because 256 characters were clearly not enough for the unique representation of 
many different characters
needed in the world's writing systems, the ISO/IEC~8859 standard in 1987
extended the MCS to include 16 different 8-bit code pages. For example, part 5
was used for Cyrillic characters, part 6 for Arabic, and part 7 for
Greek. % \footnote{In effect, because $16 = 2^4$, this means that ISO/IEC~8859 was
%actually an $8+4=12$-bit encoding, though with very many duplicates by design,
%namely all ASCII codes were repeated in each 8-bit code page. To be precise,
%ISO/IEC~8859 used the 7-bit ASCII as the basis for each code page, and defined
%16 different 7-bit extensions, leading to $(1+16) \cdot {2^7} = 2,176$ possible
%characters. However, because of overlap and not-assigned codes points the actual
%number of symbols was much smaller.} 
This system was almost immediately 
understood to be insufficient and impractical, so various initiatives to extend
and reorganize the encoding started in the 1980s. This led, for example, to
various proprietary encodings from Microsoft (e.g.~Windows Latin 1) and Apple
(e.g.~Mac OS Roman), which one still sometimes encounters today. 

In the 1980s various people started to develop true
international code sets. In the United States, a group of computer scientists
formed the \textsc{unicode consortium}, proposing a 16-bit encoding in 1991
(with 2\textsuperscript{16} = 65,536 different possible characters). At the same time in
Europe, the \textsc{international organization for standardization} (ISO) was
working on ISO~10646 to replace the ISO/IEC~8859 standard. Their first draft of
the \textsc{universal character set} (UCS) in 1990 was 31-bit (with
theoretically 2\textsuperscript{31} = 2,147,483,648 possible characters, but because of some
technical restrictions only 679,477,248 were allowed). Since 1991, the Unicode
Consortium and the ISO jointly develop the \textsc{unicode standard}, or
ISO/IEC~10646, leading to the current system including the original 16-bit
Unicode proposal as the \textsc{basic multilingual plane}, and 16 additional
planes of 16-bit for further extensions (with in total (1+16)\times 2\textsuperscript{16} =
1,114,112 possible characters). The most recent version of the Unicode Standard
(currently at version number 11.0.0) was published in June 2018 and it defines
137,374 different characters \citep{Unicode2018}.

\ 

\noindent In the next section we provide a very brief overview of the linguistic
terminology concerning writing systems before turning to the slightly different
computational terminology in the subsequent chapter on the Unicode Standard. 

% ==========================
\section{Linguistic terminology}
\label{linguistic-terminology}
% ==========================

Linguistically speaking, a \textsc{writing system} is a symbolic system that
uses visible or tactile signs to represent language in a systematic way. The
term writing system has two mutually exclusive meanings. First, it may
refer to the way a particular language is written. In this sense the term refers
to the writing system of a particular language, as, for example, in \emph{the
Serbian writing system uses two scripts: Latin and Cyrillic}. Second, the term
writing system may also refer to a type of symbolic system as, for example, in
\emph{alphabetic writing system}. In this latter sense the term refers to how
scripts have been classified according to the way that they encode language, as
in, for example, \emph{the Latin and Cyrillic scripts are both alphabetic
writing systems}. To avoid confusion, this second notion of writing system
would more aptly have been called \textsc{script system}. 

\subsubsection*{Writing systems}

Focusing on the first sense of \textsc{writing system} described above, we distinguish 
between two different kinds of writing systems used for a particular language, namely transcriptions and
orthographies. First, \textsc{transcription} is a scientific procedure (and also
the result of that procedure) for graphically representing the sounds of human
speech at the phonetic level. It incorporates a set of unambiguous symbols to
represent speech sounds, including conventions that specify how these symbols
should be combined. A transcription system is a specific system of symbols and
rules used for transcription of the sounds of a spoken language variety. In
principle, a transcription system should be language-independent, in that it
should be applicable to all spoken human languages. The \textsc{International
Phonetic Alphabet} (IPA) is a commonly used transcription system that provides a
medium for transcribing languages at the phonetic level. However, there is a
long history of alternative kinds of transcription systems
\citep[see][]{Kemp2006} and today various alternatives are in widespread use
(e.g.~X-SAMPA and Cyrillic-based phonetic transcription systems). Many
users of IPA do not follow the standard to the letter, and many dialects based
on the IPA have emerged, e.g.~the Africanist and Americanist transcription
systems. Note that IPA symbols are also often used to represent language on a
phonemic level. It is important to realize that in this usage the IPA
symbols are not a transcription system, but rather an orthography (though with
strong links to the pronunciation). Further, a transcription system does not
need to be as highly detailed as the IPA.\ It can also be a system of broad sound
classes. Although such an approximative transcription is not normally used in
linguistics, it is widespread in technological approaches
\citetext{\citealp[Soundex and variants, e.g.~][391--392]{Knuth1973};
\citealp{postel1969,Beider2008}}, and it is sometimes fruitfully used in
automatic approaches to historical linguistics
\citep{Dolgopolsky1986,List2012esslli,Brown2013}.

Second, an \textsc{orthography} specifies the symbols, punctuations, and the
rules in which a specific language is written in a standardized way.
Orthographies are often based on a phonemic analysis, but they almost always
include idiosyncrasies because of historical developments (like sound changes or
loans) and because of the widely-followed principle of lexical integrity
(i.e.~the attempt to write the same lexical root in a consistent way, also when
synchronic phonemic rules change the pronunciation, as for example with final
devoicing in many Germanic languages). Orthographies are language-specific
(and often even resource-specific), although individual symbols or rules might
be shared between languages. A \textsc{practical orthography} is a strongly
phoneme-based writing system designed for practical use by speakers. The mapping
relation between phonemes and graphemes in practical orthographies is purposely
shallow, i.e.~there is mostly a systematic and faithful mapping from a phoneme
to a grapheme. Practical orthographies are intended to jumpstart written
materials development by correlating a writing system with the sound units of a
language \citep{MeinhofJones1928}. Symbols from the IPA are often used
by linguists in the development of such practical orthographies for languages
without writing systems, though this usage of IPA symbols should not be confused
with transcription (as defined above). 

Further, a \textsc{transliteration} is a mapping between two different
orthographies. It is the process of ``recording the graphic symbols of one
writing system in terms of the corresponding graphic symbols of a second writing
system'' \citep[396]{Kemp2006}. In straightforward cases, such a transliteration
is simply a matter of replacing one symbol with another. However, there are
widespread complications, like one-to-many or many-to-many mappings, which are
not always easy, or even possible, to solve without listing all cases
individually \citep[cf.~][Ch.~2]{Moran2012}.

\subsubsection*{Script systems}

Different kinds of writing systems are classified into script
systems. A \textsc{script} is a collection of distinct symbols as
employed by one or more orthographies. For example, both Serbian and Russian are
written with subsets of the Cyrillic script. A single language, like Serbian or
Japanese, can also be written using orthographies based on different scripts.
Over the years linguists have classified script systems in a variety of ways,
with the tripartite classification of logographic, syllabic, and alphabetic
remaining the most popular, even though there are at least half a dozen
different types of script systems that can be distinguished
\citep{Daniels1990,Daniels1996}.

Breaking it down further, a script consists of \textsc{graphemes}, which are writing 
system-specific minimally distinctive symbols (see below). Graphemes may consist of one or more 
\textsc{characters}. The term `character' is overladen. In the linguistic terminology of writing
systems, a character is a general term for any self-contained element
in a writing system. A second interpretation is used as a conventional term for a unit in the Chinese writing
system \citep{Daniels1996}. In technical terminology, a character
refers to the electronic encoding of a component in a writing system that has semantic 
value (see Section \ref{character-encoding-system}). Thus in this work we must navigate 
between the general linguistic and technical terms for \textsc{character} 
and \textsc{grapheme} because of how these notions are defined and how they relate at the intersection 
between the International Phonetic Alphabet and the Unicode Standard (Chapter \ref{ipa-meets-unicode}).

Although in literate societies most people have a strong intuition
about what the characters are in their particular orthography or orthographies,
it turns out that the separation of an orthography into separate characters is
far from trivial. The widespread intuitive notion of a character is strongly
biased towards educational traditions, like the alphabet taught at schools, and
technological possibilities, like the available type pieces in a printer's job
case, the keys on a typewriter, or the symbols displayed in Microsoft Word's
symbol browser. In practice, characters often consist of multiple building
blocks, each of which could be considered a character in its own right. For
example, although a Chinese character may be considered to be a single basic
unanalyzable unit, at a more fine-grained level of analysis the internal
structure of Chinese characters is often comprised of smaller semantic and
phonetic units that should be considered characters \citep{Sproat2000}. In
alphabetic scripts, this problem is most forcefully exemplified by diacritics. 

A \textsc{diacritic} is a mark, or series of marks, that may be above, below, 
before, after, through, around, or between other characters \citep{Gaultney2002}. Diacritics are sometimes used to
distinguish homophonous words, but they are more often used to indicate a
modified pronunciation \citep[xli]{DanielsBright1996}. The central question is
whether, for example, <e>, <è>, <a> and <à> should be considered four
characters, or different combinations of three characters, i.e.\ <a>, <e>, and <\dia{0300}>. 
In general, multiple characters together can form another character, and it is not always possible to
decide on principled grounds what should be the basic building blocks of an
orthography.

For that reason, it is better to analyze an orthography as a collection of
graphemes. A \textsc{grapheme} is the basic, minimally distinctive symbol of a
particular writing system. It was modeled
after the term \textsc{phoneme} (an abstract representation of
a distinct sound in a specific language) and as such it represents a contrastive graphical unit in a
writing system (see \citealp{Kohrt1986} for a historical overview of the term
\textsc{grapheme}). Most importantly, a single grapheme regularly consists of multiple
characters, like <th>, <ou> and <gh> in English (note that each character in
these graphemes is also a separate grapheme in English). Such complex graphemes
are often used to represent single phonemes. So, a combination of characters is
used to represent a single phoneme. Note that the opposite is also found in
writing systems, in cases in which a single character represents a combination
of two or more phonemes. For example, <x> in English orthography sometimes represents a
combination of the phonemes /k/ and /s/, as in the word `index' [ˈɪnˌdɛks]; other times 
it is pronounced as /z/, such as in the words `Xerox' [ˈzɪrˌɑks]; and in `example' [ɪɡˈzæmpəl] 
it is a combination of /g/ and /s/. As one can see, there can be non-trivial mappings between graphemes and phonemes in 
orthographies like English, e.g.\ `give', `gin', `jingle', where the graphemes 
<g> and <j> and the phonemes /g/ and /dʒ/ have a complex mapping.

Further, conditioned or free variants of a grapheme are called
\textsc{allographs}. For example, the distinctive forms of Greek sigma are
conditioned, with <σ> being used word-internally and <ς> being used at the end
of a word. In sum, there are many-to-many relationships between phonemes and
graphemes as they are expressed in the myriad of language- and resource-specific
orthographies.

\subsubsection*{Summary}

\noindent This exposition of the linguistic terminology involved in describing writing
systems has been purposely brief. We have highlighted some of the linguistic
notions that are pertinent to, yet sometimes confused with, the technical
definitions developed for the computational processing of the world's writing
systems, which we describe in the next Chapter.


